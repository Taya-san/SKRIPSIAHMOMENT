{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa7d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 20 14:38:09 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0             34W /   70W |     120MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a6e6354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-20 15:05:44--  https://raw.githubusercontent.com/Taya-san/SKRIPSIAHMOMENT/main/configuration_mamba.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 950 [text/plain]\n",
      "Saving to: ‘configuration_mamba.py’\n",
      "\n",
      "configuration_mamba 100%[===================>]     950  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-20 15:05:45 (62.3 MB/s) - ‘configuration_mamba.py’ saved [950/950]\n",
      "\n",
      "--2026-01-20 15:05:45--  https://raw.githubusercontent.com/Taya-san/SKRIPSIAHMOMENT/main/modelling_mamba.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3716 (3.6K) [text/plain]\n",
      "Saving to: ‘modelling_mamba.py’\n",
      "\n",
      "modelling_mamba.py  100%[===================>]   3.63K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-20 15:05:45 (61.9 MB/s) - ‘modelling_mamba.py’ saved [3716/3716]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O configuration_mamba.py https://raw.githubusercontent.com/Taya-san/SKRIPSIAHMOMENT/main/configuration_mamba.py\n",
    "!wget -O modelling_mamba.py https://raw.githubusercontent.com/Taya-san/SKRIPSIAHMOMENT/main/modelling_mamba.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95483521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration_mamba.py\t  modelling_mamba.py\t__pycache__\n",
      "configuration_mamba.py.1  modelling_mamba.py.1\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485f7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration_mamba import MambaQuinConfig\n",
    "from modelling_mamba import MambaQuin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097f22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"mamba_quin\", MambaQuinConfig)\n",
    "AutoModel.register(MambaQuinConfig, MambaQuin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f2ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e250ea84ef64403d9c6035b9a19fcd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c0f27f419848ccbf7ad453fab03722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/517M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    }
   ],
   "source": [
    "config = MambaQuinConfig(\n",
    "    latent_dim = 64,\n",
    "    num_clusters = 5\n",
    ")\n",
    "\n",
    "model = MambaQuin(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
