{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0475dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  8 06:46:33 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260833a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu124\n",
      "Uninstalling torch-2.6.0+cu124:\n",
      "  Successfully uninstalled torch-2.6.0+cu124\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "Found existing installation: mamba-ssm 2.3.0\n",
      "Uninstalling mamba-ssm-2.3.0:\n",
      "  Successfully uninstalled mamba-ssm-2.3.0\n",
      "Found existing installation: causal-conv1d 1.6.0\n",
      "Uninstalling causal-conv1d-1.6.0:\n",
      "  Successfully uninstalled causal-conv1d-1.6.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch==2.7.0\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.25.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.10.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (75.2.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m206.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-curand-cu12/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparselt-cu12/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m159.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.11.1.6)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.7.0) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (866.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.8/866.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "Successfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0+cu126 torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126 triton-3.3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ac12a7a302bf4f079787885bd6155d10",
       "pip_warning": {
        "packages": [
         "sympy",
         "torch",
         "torchgen",
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE\n",
      "  Downloading https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.6.0/causal_conv1d-1.6.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl (287.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2.7.0+cu126)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (26.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.0.3)\n",
      "Installing collected packages: causal-conv1d\n",
      "Successfully installed causal-conv1d-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch torchvision torchaudio mamba-ssm causal-conv1d\n",
    "\n",
    "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "!pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.6.0/causal_conv1d-1.6.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495d055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE\n",
      "  Downloading https://github.com/state-spaces/mamba/releases/download/v2.3.0/mamba_ssm-2.3.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl (533.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.3/533.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.7.0+cu126)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.3.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.13.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.8.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (5.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (26.0)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (75.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.11.1.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.67.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.16.0)\n",
      "Installing collected packages: mamba-ssm\n",
      "Successfully installed mamba-ssm-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/state-spaces/mamba/releases/download/v2.3.0/mamba_ssm-2.3.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SKRIPSIAHMOMENT'...\n",
      "remote: Enumerating objects: 103, done.\u001b[K\n",
      "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 103 (delta 53), reused 76 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (103/103), 799.79 KiB | 3.46 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Taya-san/SKRIPSIAHMOMENT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63a786d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'SKRIPSIAHMOMENT/'\n",
      "/content/SKRIPSIAHMOMENT\n",
      "\u001b[0m\u001b[01;34mmamba_quin\u001b[0m/  \u001b[01;34mnotebooks+pdf\u001b[0m/  README.md  \u001b[01;34mutils\u001b[0m/\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 320 bytes | 320.00 KiB/s, done.\n",
      "From https://github.com/Taya-san/SKRIPSIAHMOMENT\n",
      "   d20b81f..90fbe42  main       -> origin/main\n",
      "Updating d20b81f..90fbe42\n",
      "Fast-forward\n",
      " utils/trainer.py | 1 \u001b[31m-\u001b[m\n",
      " 1 file changed, 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "%cd SKRIPSIAHMOMENT/\n",
    "%ls\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87a0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import sklearn.model_selection\n",
    "import mamba_ssm\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "    \n",
    "    mamba_ssm.selective_state_update = selective_state_update\n",
    "    mamba_ssm.selective_scan_fn = selective_scan_fn\n",
    "    mamba_ssm.mamba_inner_fn = mamba_inner_fn\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Patch failed! Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3f97a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_quin import MambaQuin, MambaQuinConfig\n",
    "from utils import train_modelnoclt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "097f22ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c69a43efc049a887a8ded37d109665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MambaModel LOAD REPORT from: state-spaces/mamba-130m\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "backbone.embedding.weight                                     | UNEXPECTED | \n",
      "lm_head.weight                                                | UNEXPECTED | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.in_proj.weight  | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.D               | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.out_proj.weight | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.dt_proj.bias    | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.norm.weight           | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.x_proj.weight   | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.dt_proj.weight  | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.A_log           | MISSING    | \n",
      "embeddings.weight                                             | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.conv1d.weight   | MISSING    | \n",
      "layers.{24, 25, 26, 27, 28, 29, 30, 31}.mixer.conv1d.bias     | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "config = MambaQuinConfig(\n",
    "    backbone_model = \"state-spaces/mamba-130m\",\n",
    "    num_classes = 2\n",
    ")\n",
    "\n",
    "model = AutoModel.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "049ea8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f2b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "753f7d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/391 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3654901830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_modelnoclt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/SKRIPSIAHMOMENT/utils/trainer.py\u001b[0m in \u001b[0;36mtrain_modelnoclt\u001b[0;34m(model, tokenizer, epochs, tr_data, ts_data, batch_size, optimizer_type, device, dataset_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/SKRIPSIAHMOMENT/mamba_quin/modelling_mamba.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mamba/modeling_mamba.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, cache_position, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "train_modelnoclt(model, tokenizer, 3, dataset_dict=dataset, batch_size=64, tr_data=None, ts_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d83899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33fc29c9a2741919032e94a887ff933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596496ee8b48408a941eb8d96ec9c34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  15%|█▍        | 57/391 [00:00<00:01, 283.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  22%|██▏       | 86/391 [00:00<00:01, 277.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  37%|███▋      | 144/391 [00:00<00:00, 273.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  52%|█████▏    | 203/391 [00:00<00:00, 279.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 261/391 [00:00<00:00, 280.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  82%|████████▏ | 320/391 [00:01<00:00, 283.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 391/391 [00:01<00:00, 279.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(64, 0), dtype=torch.int64)}\n",
      "{'labels': tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1], device='cuda:0'), 'input_ids': tensor([], device='cuda:0', size=(40, 0), dtype=torch.int64), 'attention_mask': tensor([], device='cuda:0', size=(40, 0), dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.optim as optim\n",
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "optimizer_type = 'adam'\n",
    "if dataset is not None :\n",
    "    training_data = dataset['train']\n",
    "    test_data = dataset['test']\n",
    "else:\n",
    "    training_data = Dataset.from_dict(tr_data)\n",
    "    test_data = Dataset.from_dict(ts_data)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenizer_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        padding = False\n",
    "    )\n",
    "\n",
    "training_data = training_data.map(\n",
    "        tokenizer_fn,\n",
    "        batched = True,\n",
    ")\n",
    "test_data = test_data.map(\n",
    "        tokenizer_fn,\n",
    "        batched = True,\n",
    ")\n",
    "\n",
    "\n",
    "training_data = training_data.rename_column('label', 'labels')\n",
    "test_data = test_data.rename_column('label', 'labels')\n",
    "\n",
    "training_data = training_data.remove_columns(['text'])\n",
    "test_data = test_data.remove_columns(['text'])\n",
    "\n",
    "training_data.set_format(\n",
    "        type = 'torch',\n",
    "        columns = ['input_ids', 'attention_mask', 'labels']\n",
    ")\n",
    "test_data.set_format(\n",
    "        type = 'torch',\n",
    "        columns = ['input_ids', 'attention_mask', 'labels']\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "        training_data,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        pin_memory = True,\n",
    "        collate_fn = collator\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False,\n",
    "        pin_memory = True,\n",
    "        collate_fn = collator\n",
    ")\n",
    "\n",
    "\n",
    "if optimizer_type == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "elif optimizer_type == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "model.float()\n",
    "model.to(device)\n",
    "\n",
    "model.config.epochs_loss_log = getattr(model.config, \"epochs_loss_log\", [])\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(training_loader, desc=f'Epoch {epoch + 1}/{1}', leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        print(batch)\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#     epoch_loss = running_loss/len(training_loader)\n",
    "#     model.config.epochs_loss_log.append(epoch_loss)\n",
    "#     print(f'Epoch {epoch + 1} loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
