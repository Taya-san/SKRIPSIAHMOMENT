{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0475dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 11 05:54:49 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260833a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.0+cu128\n",
      "Uninstalling torch-2.9.0+cu128:\n",
      "  Successfully uninstalled torch-2.9.0+cu128\n",
      "Found existing installation: torchvision 0.24.0+cu128\n",
      "Uninstalling torchvision-0.24.0+cu128:\n",
      "  Successfully uninstalled torchvision-0.24.0+cu128\n",
      "Found existing installation: torchaudio 2.9.0+cu128\n",
      "Uninstalling torchaudio-2.9.0+cu128:\n",
      "  Successfully uninstalled torchaudio-2.9.0+cu128\n",
      "\u001b[33mWARNING: Skipping mamba-ssm as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping causal-conv1d as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch==2.7.0\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.25.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.10.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m215.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m224.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-curand-cu12/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m188.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparselt-cu12/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cufile-cu12/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m201.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.7.0) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (866.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m866.8/866.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.13.1.3\n",
      "    Uninstalling nvidia-cufile-cu12-1.13.1.3:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "Successfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.7.0+cu126 torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126 triton-3.3.0\n",
      "Collecting causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE\n",
      "  Downloading https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.6.0/causal_conv1d-1.6.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl (287.4 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m287.4/287.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2.7.0+cu126)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (26.0)\n",
      "Collecting ninja (from causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->causal-conv1d==1.6.0+cu12torch2.7cxx11abiFALSE) (3.0.3)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja, causal-conv1d\n",
      "Successfully installed causal-conv1d-1.6.0 ninja-1.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch torchvision torchaudio mamba-ssm causal-conv1d\n",
    "\n",
    "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "!pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.6.0/causal_conv1d-1.6.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495d055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE\n",
      "  Downloading https://github.com/state-spaces/mamba/releases/download/v2.3.0/mamba_ssm-2.3.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl (533.3 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m533.3/533.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.7.0+cu126)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.3.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.13.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.8.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (5.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (26.0)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (75.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.11.1.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.67.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers->mamba-ssm==2.3.0+cu12torch2.7cxx11abiFALSE) (0.16.0)\n",
      "Installing collected packages: mamba-ssm\n",
      "Successfully installed mamba-ssm-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/state-spaces/mamba/releases/download/v2.3.0/mamba_ssm-2.3.0+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab80d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/SKRIPSIAHMOMENT_colab\n",
      "mamba_quin  notebooks+pdf  README.md  sync_to_colab.sh\tutils\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount the Drive (The boring part)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. The Magic Shortcut 笨ｨ\n",
    "# This makes your deep folder appear right at /content/project\n",
    "my_drive_path = '/content/drive/MyDrive/SKRIPSIAHMOMENT_colab'\n",
    "shortcut_path = '/content/SKRIPSIAHMOMENT_colab'\n",
    "\n",
    "if not os.path.exists(shortcut_path):\n",
    "    os.symlink(my_drive_path, shortcut_path)\n",
    "    print(f\"沐 Linked! Your files are now at: {shortcut_path}\")\n",
    "\n",
    "# 3. Enter the zone\n",
    "%cd {shortcut_path}\n",
    "!ls  # Verify you see your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SKRIPSIAHMOMENT'...\n",
      "remote: Enumerating objects: 161, done.\u001b[K\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
      "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
      "remote: Total 161 (delta 97), reused 111 (delta 47), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (161/161), 820.41 KiB | 3.48 MiB/s, done.\n",
      "Resolving deltas: 100% (97/97), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Taya-san/SKRIPSIAHMOMENT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a786d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/SKRIPSIAHMOMENT\n",
      "\u001b[0m\u001b[01;34mmamba_quin\u001b[0m/  \u001b[01;34mnotebooks+pdf\u001b[0m/  README.md  \u001b[01;34mutils\u001b[0m/\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%cd SKRIPSIAHMOMENT/\n",
    "%ls\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, MambaConfig, MambaModel\n",
    "from datasets import load_dataset\n",
    "import sklearn.model_selection\n",
    "import mamba_ssm\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "    \n",
    "    mamba_ssm.selective_state_update = selective_state_update\n",
    "    mamba_ssm.selective_scan_fn = selective_scan_fn\n",
    "    mamba_ssm.mamba_inner_fn = mamba_inner_fn\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Patch failed! Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f97a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_quin import MambaQuin, MambaQuinConfig\n",
    "from utils import train_modelnoclt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f37757758de40fe9ee9bc312cc02e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_backbone = MambaModel.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "pretrained_backbone_config = MambaConfig.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "config = MambaQuinConfig(\n",
    "    backbone_config = pretrained_backbone_config.to_dict(),\n",
    "    num_classes = 2\n",
    ")\n",
    "\n",
    "model = AutoModel.from_config(config)\n",
    "model.backbone.load_state_dict(pretrained_backbone.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049ea8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f2b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753f7d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08b1e08be8d4392919ac6dc2cb0bce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 3125/3125 [09:24<00:00,  5.54it/s, loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.5419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_modelnoclt(model, tokenizer, 1, dataset_dict=dataset, batch_size=8, tr_data=None, ts_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91769f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b67f3bfedbe4907a3eb4bf58261da69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b94e10b6e14f9fbfe550f143b2057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43e5d808ce942eabccf7d2a47e303a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a326cf5ef6744308917390143519382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...qb_8fzi/model.safetensors:   3%|3         | 16.7MB /  520MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/GiganticLemon/mambaquin-small/commit/49b93ecede850acf0e568642eff490b17cc2302e', commit_message='Upload tokenizer', commit_description='', oid='49b93ecede850acf0e568642eff490b17cc2302e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/GiganticLemon/mambaquin-small', endpoint='https://huggingface.co', repo_type='model', repo_id='GiganticLemon/mambaquin-small'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"GiganticLemon/mambaquin-small\")\n",
    "tokenizer.push_to_hub(\"GiganticLemon/mambaquin-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c956c1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07218f2c1bcf4c9fa749fde8140b2c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62543f60f72c438cbd416d9854448d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/520M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7abdba68e644deb82b66c5959868a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'MambaQuin' object has no attribute 'all_tied_weights_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-185885155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_test = AutoModel.from_pretrained(\"GiganticLemon/mambaquin-small\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                                        trust_remote_code = True)\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GiganticLemon/mambaquin-small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         )\n\u001b[1;32m   4109\u001b[0m         \u001b[0mload_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0mload_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_load_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_use_kernels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_kernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_finalize_load_state_dict\u001b[0;34m(model, load_config, load_info)\u001b[0m\n\u001b[1;32m   4262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4263\u001b[0m         \u001b[0;31m# Marks tied weights as `_is_hf_initialized` to avoid initializing them (it's very important for efficiency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4264\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_tied_weights_as_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m         \u001b[0;31m# Move missing (and potentially mismatched) keys and non-persistent buffers back to their expected device from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mmark_tied_weights_as_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4585\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mvery\u001b[0m \u001b[0mimportant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmost\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0mare\u001b[0m \u001b[0mtied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mhuge\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvocabularies\u001b[0m \u001b[0mare\u001b[0m \u001b[0moften\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4586\u001b[0m         running inits on them is very costly.\"\"\"\n\u001b[0;32m-> 4587\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtied_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tied_weights_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4588\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtied_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4589\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hf_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1941\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MambaQuin' object has no attribute 'all_tied_weights_keys'"
     ]
    }
   ],
   "source": [
    "model_test = AutoModel.from_pretrained(\"GiganticLemon/mambaquin-small\",\n",
    "                                       trust_remote_code = True)\n",
    "tokenizer_test = AutoModel.from_pretrained(\"GiganticLemon/mambaquin-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb55d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72e983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  9 16:01:57 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
